{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Assessed Coursework</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this iPython notebook, I will be looking at opinion extraction. Opinion extraction, also know as sentiment analysis is the process of using computing machines to derrive sentiment and opinions about various topics, including, although not limited to: product reviews, political policies or even public opinion on a sports team's perfomance.</p>\n",
    "<p>Over the last few weeks I have completed work in the lab in which I have built an opinion extractor but incrementally adding functionality in the form of extensions. I will now show my code for my opinion extractor and the exetentions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Jamie Mayer\\Documents\\NLE\\LanguageEngineering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code cells are used to import the NLTK into iPython. I have used the first one for working in the lab and the second one when working from home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Opinion Extractor</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main Opionion Extractor Function. The main function that a parsed sentence in passed in to\n",
    "# along with an aspect token, e.g plot. Calls other functions in order to extract opinions \n",
    "# from the parsed sentence in relation to the aspect. Returns a list of opinions in the form of \n",
    "# strings.\n",
    "def opinion_extractor(aspect_token, parsed_sentence):\n",
    "    \n",
    "    opinions = [] # List of strings that will ultimately be returned.\n",
    "    \n",
    "    opinions += get_amod(aspect_token, parsed_sentence) #calls the get_amod function that I added in Extension 1.\n",
    "    \n",
    "    opinions += get_copulae(aspect_token, parsed_sentence)\n",
    "    \n",
    "    opinions += get_conjunctions(aspect_token, parsed_sentence)\n",
    "\n",
    "    return opinions\n",
    "\n",
    "\n",
    "# get_amod function. Called inside the main opinion extractor function. Takes an aspect token and a parsed sentence.\n",
    "# returns opinions, a list of strings.\n",
    "def get_amod(aspect_token, parsed_sentence):\n",
    "    opinions = [] #List of strings that the function will return.\n",
    "    \n",
    "    for dependant in parsed_sentence.get_dependants(aspect_token): # for every dependant of the aspect token:\n",
    "        # if the dependant's relation on the aspect token is that of an adjectival modifier:\n",
    "        if dependant.deprel == 'amod' and dependant.form != 'main' and dependant.form != 'special': \n",
    "            # makes a call to get_advmod to find if the dependant has an adverbial modifier and if it does\n",
    "            # stores it in the variable 'adv'.\n",
    "            adv = get_advmod(dependant, parsed_sentence)\n",
    "            jj = dependant.form #the form of the dependant stored as a string.\n",
    "            \n",
    "            # makes a call to the get_neg function, passing in the aspect token and the parsed sentence\n",
    "            # to find out if the opinion contains any negation.\n",
    "            neg = get_neg(aspect_token, parsed_sentence)\n",
    "            \n",
    "            # if the call to get_advmod did not return an empty string:\n",
    "            if adv != '':\n",
    "                # concatinate 'neg' 'adv' and 'jj' into one string, hypenating 'adv and 'jj' and append it to 'opinions'\n",
    "                opinions.append(neg + adv + '-' + jj)\n",
    "            else:\n",
    "                opinions.append(neg + jj) #concatinate 'neg' and 'jj' into one string and append it to 'opinions'\n",
    "                 \n",
    "    return opinions\n",
    "\n",
    "#get_copulae function. Called inside main opinion extractor function. Takes an aspect token and a parsed sentence.\n",
    "# Returns a list of copulae. List could be empty.\n",
    "\n",
    "def get_copulae(aspect_token, parsed_sentence):\n",
    "    copulae = [] #initialise list of copulae that will be returned.\n",
    "    neg = '' # initialise the variable 'neg' to contain empty string.    \n",
    "    # If the aspect token has dependency relation of 'nsubj' on it's head and the head is pos tagged as an adjective:\n",
    "    if aspect_token.deprel == 'nsubj' and parsed_sentence.get_head(aspect_token).pos =='JJ':\n",
    "            # Makes a call to get_advmod, passing the head of the aspect token and the parsed sentence to find if the\n",
    "            # head of the aspect token has any adjectival modifiers.\n",
    "            adv = get_advmod(parsed_sentence.get_head(aspect_token), parsed_sentence)            \n",
    "            jj = parsed_sentence.get_head(aspect_token).form # stores the form of the head of the aspect token in 'jj'\n",
    "            # Makes a call to get_neg, passing in the head of the aspect token and the parsed sentence. Checks if\n",
    "            # the opinion contains any negation.\n",
    "            neg = get_neg(parsed_sentence.get_head(aspect_token), parsed_sentence)\n",
    "            # If 'adv' is not an empty string:\n",
    "            if adv != '':\n",
    "                    # Concatinate 'neg', 'adv' and 'jj' into one string, hypenating 'adv' and 'jj'\n",
    "                    # and append it to 'copulae'\n",
    "                    copulae.append(neg + adv + '-' + jj)\n",
    "            else:\n",
    "                    # Else concatinate 'neg' and 'jj' into one string and append to 'copulae'.\n",
    "                    copulae.append(neg + jj)\n",
    "    \n",
    "    return copulae\n",
    "\n",
    "# get_neg function. Takes a basic token object and a parsed sentence. Looks for negation in relation to the exressed\n",
    "# opinion.\n",
    "def get_neg(adj_token, parsed_sentence):            \n",
    "    # for each dependant of 'adj_token':\n",
    "    for dependant in parsed_sentence.get_dependants(adj_token):\n",
    "        # if the dependant has a dependency relation of 'neg'\n",
    "        if dependant.deprel == 'neg':\n",
    "            return 'not ' # Return the string 'not'\n",
    "    #else return the empty string\n",
    "    return ''\n",
    "\n",
    "# get_advmod function. Takes a basic token object and a parsed sentence. Looks for an adverbial modifier in relation to the\n",
    "# expressed opinion\n",
    "def get_advmod(adj_token, parsed_sentence):\n",
    "    # for each token in the parsed sentence:\n",
    "    for token in parsed_sentence:\n",
    "        # if the dependency relation of the token is 'advmod' and the head is the token pased in:\n",
    "        if token.deprel == 'advmod' and token.head == adj_token.id:\n",
    "            #return that token.\n",
    "            return token.form\n",
    "    return ''\n",
    "\n",
    "# get_conjunctions function. Takes a basic token object and a parsed sentence. Looks for conjuncts in parsed sentences.\n",
    "def get_conjunctions(aspect_token, parsed_sentence):\n",
    "    conjunctions = [] # Make an empty list called 'conjunctions'.\n",
    "    # get the head of the token passed in. Get the dependants of said head. Then, for each token:\n",
    "    for dependant in parsed_sentence.get_dependants(parsed_sentence.get_head(aspect_token)):\n",
    "        # if the token has a dependency relation of 'conj':\n",
    "        if dependant.deprel == 'conj':\n",
    "            # call get_admov to see if the depenant has an adverbial modifier associtated with it and store the result in \n",
    "            # 'adv'\n",
    "            adv = get_advmod(dependant, parsed_sentence)\n",
    "            # call get_neg to find any negation that may be present in the opinion and store the result in 'neg'\n",
    "            neg = get_neg(dependant, parsed_sentence)\n",
    "            if adv != '': # Then if 'adv' does not contain the empty string:\n",
    "                conjunctions.append(neg + adv + ' ' + dependant.form) # concatinate 'neg', 'adv' a single whitespace\n",
    "                                                                      # character and the form of the dependant. And\n",
    "                                                                      # and append it to 'conjunctions'\n",
    "            else:\n",
    "                # else if 'adv' does contain the empty strong then only append 'neg' and the form of the dependant and \n",
    "                # append it to the list 'conjunctions'\n",
    "                conjunctions.append(neg + dependant.form)\n",
    "    return conjunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell will load the example sentences test set used in the extentions from labs 8 & 9 and store them in a list, parsed_example_sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.parse import load_parsed_example_sentences\n",
    "\n",
    "parsed_example_sentences = load_parsed_example_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now demonstrate that my opinion extractor produces the correct output for the examples I have been given for each of the extensions. As the example sentences are all regarding the plot I will only be testing the extentions using the plot aspect for the example sentences test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Extension 1: Adjectival Modification</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In extension 1 the objective was to tackle adjectival modification. I achevied this using the get_amod function. Of the 16 parsed sentences given in the example sentences testing set, the first 2 sentences are directly related to this extension and as such are the two sentences that I will focus on for this part of my report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tIt\tPRP\t2\tnsubj\n",
      "2\thas\tVBZ\t0\troot\n",
      "3\tan\tDT\t6\tdet\n",
      "4\texciting\tJJ\t6\tamod\n",
      "5\tfresh\tJJ\t6\tamod\n",
      "6\tplot\tNN\t2\tdobj\n",
      "7\t.\t.\t2\tpunct\n",
      "-------------------------------------\n",
      "plot is ['exciting', 'fresh'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'plot'\n",
    "# for parsed sentences in parsed_example_sentences up to the index 1 (as we are only concerned with the first sentence\n",
    "# for extension 1):\n",
    "for parsed_sentence in parsed_example_sentences[:1]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s is %s \\n\\n' % (aspect, opinion) #print 'aspect is opinion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above code output shows, my opinion extractor clearly works for ajdectival modification. The opinion in the sentence states that 'It has an exciting, fresh plot.'\n",
    "\n",
    "\n",
    "Therefore 'exciting' and 'fresh' should be extracted.\n",
    "\n",
    "The fact that my extractor does this shows that it is functioning correctly for the example sentences test set for extension 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Extension 2: Adjectives Linked by Copulae</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension 2 was all about finding adjectives linked to the aspect work via couplae. For example, if the opinion extractor only had Extension 1 functionality, then the parsed sentence 'the plot was dull.', would not return 'dull' as it relies on a copula. The second sentence in the example test set concerns copulae and so I will now put it through the opinion extractor to show that extension 2 is functioning as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t4\tnsubj\n",
      "3\twas\tVBD\t4\tcop\n",
      "4\tdull\tJJ\t0\troot\n",
      "5\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "plot is ['dull'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'plot'\n",
    "# for parsed sentences in parsed_example_sentences from index 1 up to index 2 (as we are only concerned with the second\n",
    "# sentence for extension 2):\n",
    "for parsed_sentence in parsed_example_sentences[1:2]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s is %s \\n\\n' % (aspect, opinion) #print 'aspect is opinion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the abouve code shows that for the second sentence in the example testing set, the opinion extractor is displaying the correct behavior. The token 'dull' has been extracted using the copula 'was'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Extension 3: Adverbial Modifiers</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In extension 3, we were to look at adverbial modifiers. This adresses situations where an adjective is modified by an adverb. When an adverb modifies a verb in a sentencem it will have the dependency relation of 'advmod'. The extension that I have added to the opinion extractor in this section will be able to extract adverbial modifiers and output them in the list of opinions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tIt\tPRP\t2\tnsubj\n",
      "2\thas\tVBZ\t0\troot\n",
      "3\tan\tDT\t6\tdet\n",
      "4\texcessively\tRB\t5\tadvmod\n",
      "5\tdull\tJJ\t6\tamod\n",
      "6\tplot\tNN\t2\tdobj\n",
      "7\t.\t.\t2\tpunct\n",
      "-------------------------------------\n",
      "plot is ['excessively-dull'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t5\tnsubj\n",
      "3\twas\tVBD\t5\tcop\n",
      "4\texcessively\tRB\t5\tadvmod\n",
      "5\tdull\tJJ\t0\troot\n",
      "6\t.\t.\t5\tpunct\n",
      "-------------------------------------\n",
      "plot is ['excessively-dull'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'plot'\n",
    "# for parsed sentences in parsed_example_sentences from index 2 up to index 4 (as we are only concerned with the second and\n",
    "# third sentences for extension 3):\n",
    "for parsed_sentence in parsed_example_sentences[2:4]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s is %s \\n\\n' % (aspect, opinion) #print 'aspect is opinion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the output above. The token 'excessively' has an 'advmod' relation with it's head being the adjective 'dull' which in turn is dependant on the aspect 'plot'. The output 'excessively-dull'. This shows that my opinion extractor is working for the third sentence in the example testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Extension 4: Negation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extensions already discussed are unable to cope with any form of negation. For example if the parsed sentence, 'the plot wasn't dull' was passed into the opinion extractor with only extensions 1-3 implemented, then the opinion 'dull' would be extracted, which is entirely inaccurate. This is because the sentence contains negation. The code below will run the 4th sentence through the opinion extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t5\tnsubj\n",
      "3\twas\tVBD\t5\tcop\n",
      "4\tn't\tRB\t5\tneg\n",
      "5\tdull\tJJ\t0\troot\n",
      "6\t.\t.\t5\tpunct\n",
      "-------------------------------------\n",
      "plot is ['not dull'] \n",
      "\n",
      "\n",
      "1\tIt\tPRP\t7\tnsubj\n",
      "2\twas\tVBD\t7\tcop\n",
      "3\tn't\tRB\t7\tneg\n",
      "4\tan\tDT\t7\tdet\n",
      "5\texciting\tJJ\t7\tamod\n",
      "6\tfresh\tJJ\t7\tamod\n",
      "7\tplot\tNN\t0\troot\n",
      "8\t.\t.\t7\tpunct\n",
      "-------------------------------------\n",
      "plot is ['not exciting', 'not fresh'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t6\tnsubj\n",
      "3\twas\tVBD\t6\tcop\n",
      "4\tn't\tRB\t6\tneg\n",
      "5\texcessively\tRB\t6\tadvmod\n",
      "6\tdull\tJJ\t0\troot\n",
      "7\t.\t.\t6\tpunct\n",
      "-------------------------------------\n",
      "plot is ['not excessively-dull'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'plot'\n",
    "# for parsed sentences in parsed_example_sentences from index 4 up to index 7.\n",
    "for parsed_sentence in parsed_example_sentences[4:7]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s is %s \\n\\n' % (aspect, opinion) #print 'aspect is opinion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My negation function has taken the sentence and correctly identified that the opinion is that the plot was not dull. If the function identies negation will add the word 'not' to the front of the adjective. In this case 'not dull'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Extension 5: Conjunction</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conjunction is used to connect words in a sentence. For example in the sentence 'The plot was cheesy, but fun and inspiring.', 'fun' and 'cheesy' are both conjoined with 'cheesy' which istelf is dependent on the aspect 'plot'. This means that they both also apply to the aspect. By adding extension 5, I hope to add the functionality to my opinion extractor, making it able to extract conjuncts as a part of the opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t4\tnsubj\n",
      "3\twas\tVBD\t4\tcop\n",
      "4\tcheesy\tJJ\t0\troot\n",
      "5\t,\t,\t4\tpunct\n",
      "6\tbut\tCC\t4\tcc\n",
      "7\tfun\tNN\t4\tconj\n",
      "8\tand\tCC\t4\tcc\n",
      "9\tinspiring\tVBG\t4\tconj\n",
      "10\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "plot is ['cheesy', 'fun', 'inspiring'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t5\tnsubj\n",
      "3\twas\tVBD\t5\tcop\n",
      "4\treally\tRB\t5\tadvmod\n",
      "5\tcheesy\tJJ\t0\troot\n",
      "6\tand\tCC\t5\tcc\n",
      "7\tnot\tRB\t9\tneg\n",
      "8\tparticularly\tRB\t9\tadvmod\n",
      "9\tspecial\tJJ\t5\tconj\n",
      "10\t.\t.\t5\tpunct\n",
      "-------------------------------------\n",
      "plot is ['really-cheesy', 'not particularly special'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'plot'\n",
    "# for parsed sentences in parsed_example_sentences from index 7 up to index 9. \n",
    "\n",
    "for parsed_sentence in parsed_example_sentences[7:9]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s is %s \\n\\n' % (aspect, opinion) #print 'aspect is opinion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the code shows that, when applying my opinion extractor to the testing set of sentences provided, extension 5 has been implemented successfully. Further evidence of the negation function displaying the correct behavoir can also be seen in the second sentence by successfully extracting the opinion of the plot being 'not particularly special'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have shown that extensions 1 to 5 are working and behaving as expected, however my opinion extractor does have some limitations which, had I implemented the additional extensions. I will now run the final example sentences from the testing set through my opinion extractor to highlight the limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tDT\t2\tdet\n",
      "2\tscript\tNN\t7\tnsubj\n",
      "3\tand\tCC\t2\tcc\n",
      "4\tplot\tNN\t2\tconj\n",
      "5\tare\tVBP\t7\tcop\n",
      "6\tutterly\tRB\t7\tadvmod\n",
      "7\texcellent\tJJ\t0\troot\n",
      "8\t.\t.\t7\tpunct\n",
      "-------------------------------------\n",
      "plot is ['plot'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tscript\tNN\t6\tnsubj\n",
      "3\tand\tCC\t2\tcc\n",
      "4\tplot\tNN\t2\tconj\n",
      "5\twere\tVBD\t6\tcop\n",
      "6\tunoriginal\tJJ\t0\troot\n",
      "7\tand\tCC\t6\tcc\n",
      "8\tboring\tVBG\t6\tconj\n",
      "9\t.\t.\t6\tpunct\n",
      "-------------------------------------\n",
      "plot is ['plot'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t5\tnsubj\n",
      "3\twas\tVBD\t5\taux\n",
      "4\tn't\tRB\t5\tneg\n",
      "5\tlacking\tVBG\t0\troot\n",
      "6\t.\t.\t5\tpunct\n",
      "-------------------------------------\n",
      "plot is [] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t4\tnsubj\n",
      "3\tis\tVBZ\t4\tcop\n",
      "4\tfull\tJJ\t0\troot\n",
      "5\tof\tIN\t4\tprep\n",
      "6\tholes\tNNS\t5\tpobj\n",
      "7\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "plot is ['full'] \n",
      "\n",
      "\n",
      "1\tThere\tEX\t2\texpl\n",
      "2\twas\tVBD\t0\troot\n",
      "3\tno\tDT\t5\tdet\n",
      "4\tlogical\tJJ\t5\tamod\n",
      "5\tplot\tNN\t2\tnsubj\n",
      "6\tto\tTO\t5\tprep\n",
      "7\tthis\tDT\t8\tdet\n",
      "8\tstory\tNN\t6\tpobj\n",
      "9\t.\t.\t2\tpunct\n",
      "-------------------------------------\n",
      "plot is ['logical'] \n",
      "\n",
      "\n",
      "1\tI\tPRP\t2\tnsubj\n",
      "2\tloved\tVBD\t0\troot\n",
      "3\tthe\tDT\t4\tdet\n",
      "4\tplot\tNN\t2\tdobj\n",
      "5\t.\t.\t2\tpunct\n",
      "-------------------------------------\n",
      "plot is [] \n",
      "\n",
      "\n",
      "1\tI\tPRP\t4\tnsubj\n",
      "2\tdid\tVBD\t4\taux\n",
      "3\tn't\tRB\t4\tneg\n",
      "4\tmind\tVB\t0\troot\n",
      "5\tthe\tDT\t6\tdet\n",
      "6\tplot\tNN\t4\tdobj\n",
      "7\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "plot is [] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'plot'\n",
    "# for parsed sentences in parsed_example_sentences from index 7 up to index 9. \n",
    "\n",
    "for parsed_sentence in parsed_example_sentences[9:]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s is %s \\n\\n' % (aspect, opinion) #print 'aspect is opinion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first two examples, the aspect 'plot' is conjunct on another aspect 'script'. Although I have addressed conjunction in Extension 5, it has been implemented with the intent of being applied to adjectives dependent on the aspect rather than the aspect it self being a conjunction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b> Applying Opinion Extractor to Amazon DVD Corpus</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now attempt to assess the limitations of my opinion extractor when applied to the sentences in the corpus of Amazon DVD reviews that I have been provided with.\n",
    "\n",
    "One of the parameters that my opinion extractor takes is a parsed sentence, which means that the sentences I am going to analyse must first be run through a parser before they can be passed to the opinion extractor. This is turn means that they must be Part of Speech (POS) tagged in order to be parsed. \n",
    "\n",
    "Before I do either of these things I will first go into more depth about what they are and what they involve. First I will discuss POS tagging.\n",
    "\n",
    "<b> Part of Speech Tagging</b>\n",
    "\n",
    "\n",
    "POS tagging is the proccess of categorising words according to their gramatical function. These can fall into one of many categories, the main ones of which being:\n",
    "\n",
    "\n",
    "                                            Adjective\n",
    "                                            Noun\n",
    "                                            Verb\n",
    "                                            Adverb\n",
    "                                            Conjunction\n",
    "                                            Pronoun\n",
    "                                            Interjection\n",
    "                                            Preposition\n",
    "                                            Determiner\n",
    "                                     \n",
    "\n",
    "POS tags are not universal and there exist differing sets of POS tags, for example the Penn Tree bank contains 45 tags, the Brown corpus contains 87 and the C7 tagset contains 146. Jurafsky and Martin (2009) explain that the added granularity that comes with a greater number of tags can be rather useful, using the example of distinguishing between possessive pronouns and personal pronouns they say that knowing this information is a key factor in predicting other words that are likely to crop up around the word in question.\n",
    "\n",
    "POS tagging serves multiple purposes, mainly those of (i) disambiguating words, (ii) prediction of what words may preceed or follow a certain word and (iii) for helping to spot gramatical structure in the bigger picture, this last purpose is very related to parsing which I shall discuss in due course. \n",
    "\n",
    "POS tags can be split up into 2 classes, these are the closed classes and the open classes. Closed classes get their name from the fact that there is a fixed set of them, implying that they are very rarely (if ever), added to. They do not convey any meaning or posses any content but are of vital importance gramatically as they form the backbone of the main structure of the sentence. Words like 'the', 'by' and 'and' belong to a closed POS classes (article, preposition and conjunction respectively). They occur frequently and are usually relatively short in length. In contrast open classses are, as their name suggests more open in that they are added to frequently. The open classes contain the main content and will generally be associated with the actual meaning of the sentence. Another interesting point raised by Jurafsky and Martin in the same text is that there are four major open word classes in world languages;\n",
    "\n",
    "                                            Nouns\n",
    "                                            Verbs\n",
    "                                            Adjectives\n",
    "                                            Adverbs\n",
    "\n",
    "The interesting thing that they remark upon is that English contains all of these classes, however not all languages do. As mentioned previously, these classes can be further categorised into sub classes, for example a Noun could be of the class, porper noun, common noun, mass noun etc. \n",
    "                                            \n",
    "\n",
    "So when a sentence is POS tagged, each word in that sentence is assigned its own tag, e.g adjective. Although this is not an easy task as one word could have high ambiguity, meaning that a single word could have different meanings and thus a different POS tag depending on its context. For example the word 'milk' could be a noun or a verb. In the sentence 'Would you like milk with that?' it is easy for a human to see that milk is being used in the context of a noun. However the same word is used in the sentence 'Don't milk it' where it is used as an adjective. Often an ambigious word is a lot more likely to have one tag than another, for example a word could occur as a noun 98% of the time while the other 2% of the time it could appear as an adjective. \n",
    "\n",
    "A POS tag often has an abrevation associated with it, i.e a noun could be 'NN', an adjective 'JJ', a determiner 'DET' etc. Once a sentence has been POS tagged it can then be dependency parsed. \n",
    "\n",
    "\n",
    "\n",
    "<b>Dependency Parsing</b>\n",
    "\n",
    "\n",
    "Dependency parsing is type of automated analysis of syntax that relies upon the idea that words are related to each other by directed links called dependencies. If word 'a' is dependant on word 'b' then 'a' is said to be the dependant and 'b' is said to be the head of that dependant. Dependancy Parsing uses binary relations, that is to say a single relation only occurs between 2 words. However one word can have multiple dependants but must always only have one head. During the parsing, an artificial token as added called 'root'. Kübler,S et al. (2009), explain how this is done in order to allow us to form tighter formal definitions, in that we can say all tokens must always have a head, even if that head is an artificial word that has been added and bears no meaning, context or even adds anything to the sentence at all. The attribute of dependency relation describes the type of relationship between the head and its dependant. For example a relation of 'advmod' describes an adverbial modification relation type, where the dependant is an adverb that modifies its head. When applied to the entire sentence, a structure of dependency/head relations is formed, an example of which can be seen below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](./images/Capture3.jpg)\n",
    "<center>Kübler,S et al. (2009)</center>\n",
    "\n",
    "<br> The example illustrates a parsed sentence's dependency structure. Taking the word 'had' as an example, it's head is 'root' and it has 3 dependants, one of which is the word 'news'. News has a dependency type 'subject' and in is the head of the word 'economic' which has a dependency relation of attribute. The above dependency tree illustrates how a dependency structure of a sentence visually, with each word having a head. The arrows always go from head to dependant\n",
    "\n",
    "<br><br>When a sentence has been dependency parsed each word is labeled with the following attributes;\n",
    "\n",
    "                                            ID -  A unique identifer local to the sentence\n",
    "                                            \n",
    "                                            Form - A string containing the actual word\n",
    "                                            \n",
    "                                            POS tag - The POS tag of the word\n",
    "                                            \n",
    "                                            Head - The head in that word's dependency relation\n",
    "                                            \n",
    "                                            Dependency Relation - The relation type of the word to its head\n",
    "                                            \n",
    "When a sentence has been parsed, it can then be used in the opinion extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I will now run sample sets of reviews from the Amazon DVD corpus through my opinion extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.parse import load_parsed_dvd_sentences\n",
    "\n",
    "parsed_sentences_plot = load_parsed_dvd_sentences('plot')\n",
    "\n",
    "parsed_sentences_characters = load_parsed_dvd_sentences('characters')\n",
    "\n",
    "parsed_sentences_cinematography = load_parsed_dvd_sentences('cinematography')\n",
    "\n",
    "parsed_sentences_dialogue = load_parsed_dvd_sentences('dialogue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t5\tnsubj\n",
      "3\twas\tVBD\t5\tcop\n",
      "4\tless\tJJR\t5\tamod\n",
      "5\tcoherent\tNN\t0\troot\n",
      "6\t,\t,\t5\tpunct\n",
      "7\tbut\tCC\t5\tcc\n",
      "8\tif\tIN\t10\tmark\n",
      "9\tyou\tPRP\t10\tnsubj\n",
      "10\twatch\tVBP\t21\tadvcl\n",
      "11\tflicks\tNNS\t10\tdobj\n",
      "12\tlike\tIN\t10\tprep\n",
      "13\tthis\tDT\t12\tpobj\n",
      "14\tfor\tIN\t13\tprep\n",
      "15\tthe\tDT\t16\tdet\n",
      "16\tplot\tNN\t14\tpobj\n",
      "17\t,\t,\t21\tpunct\n",
      "18\tsomething\tNN\t21\tnsubj\n",
      "19\tis\tVBZ\t21\tcop\n",
      "20\tclinically\tRB\t21\tadvmod\n",
      "21\twrong\tJJ\t5\tconj\n",
      "22\twith\tIN\t21\tprep\n",
      "23\tyou\tPRP\t22\tpobj\n",
      "24\t.\t.\t5\tpunct\n",
      "-------------------------------------\n",
      "plot is ['clinically wrong'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t5\tnsubj\n",
      "3\twas\tVBD\t5\tcop\n",
      "4\tless\tJJR\t5\tamod\n",
      "5\tcoherent\tNN\t0\troot\n",
      "6\t,\t,\t5\tpunct\n",
      "7\tbut\tCC\t5\tcc\n",
      "8\tif\tIN\t10\tmark\n",
      "9\tyou\tPRP\t10\tnsubj\n",
      "10\twatch\tVBP\t21\tadvcl\n",
      "11\tflicks\tNNS\t10\tdobj\n",
      "12\tlike\tIN\t10\tprep\n",
      "13\tthis\tDT\t12\tpobj\n",
      "14\tfor\tIN\t13\tprep\n",
      "15\tthe\tDT\t16\tdet\n",
      "16\tplot\tNN\t14\tpobj\n",
      "17\t,\t,\t21\tpunct\n",
      "18\tsomething\tNN\t21\tnsubj\n",
      "19\tis\tVBZ\t21\tcop\n",
      "20\tclinically\tRB\t21\tadvmod\n",
      "21\twrong\tJJ\t5\tconj\n",
      "22\twith\tIN\t21\tprep\n",
      "23\tyou\tPRP\t22\tpobj\n",
      "24\t.\t.\t5\tpunct\n",
      "-------------------------------------\n",
      "plot is [] \n",
      "\n",
      "\n",
      "1\tExplaining\tVBG\t9\tcsubj\n",
      "2\tthe\tDT\t3\tdet\n",
      "3\tplot\tNN\t1\tdobj\n",
      "4\tto\tTO\t1\tprep\n",
      "5\tyou\tPRP\t4\tpobj\n",
      "6\twould\tMD\t9\taux\n",
      "7\tprobably\tRB\t9\tadvmod\n",
      "8\tbe\tVB\t9\tcop\n",
      "9\tpointless\tJJ\t0\troot\n",
      "10\t.\t.\t9\tpunct\n",
      "-------------------------------------\n",
      "plot is [] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t3\tdet\n",
      "2\tmovie\tNN\t3\tnn\n",
      "3\tplot\tNN\t8\tnsubj\n",
      "4\tis\tVBZ\t8\tcop\n",
      "5\tvery\tRB\t6\tadvmod\n",
      "6\tmuch\tJJ\t8\tamod\n",
      "7\tthe\tDT\t8\tdet\n",
      "8\tsame\tJJ\t0\troot\n",
      "9\tas\tIN\t8\tprep\n",
      "10\tthe\tDT\t11\tdet\n",
      "11\tstory\tNN\t9\tpobj\n",
      "12\tof\tIN\t11\tprep\n",
      "13\tthe\tDT\t16\tdet\n",
      "14\tfirst\tJJ\t16\tamod\n",
      "15\tthree\tCD\t16\tnum\n",
      "16\tbooks\tNNS\t12\tpobj\n",
      "17\t.\t.\t8\tpunct\n",
      "-------------------------------------\n",
      "plot is ['same'] \n",
      "\n",
      "\n",
      "1\tAlong\tIN\t0\troot\n",
      "2\tthe\tDT\t3\tdet\n",
      "3\tway\tNN\t1\tpobj\n",
      "4\t,\t,\t0\troot\n",
      "5\tThornhill\tNNP\t6\tnn\n",
      "6\tmeets\tNNS\t0\troot\n",
      "7\ta\tDT\t9\tdet\n",
      "8\tbeautiful\tJJ\t9\tamod\n",
      "9\tblond\tNN\t10\tnsubj\n",
      "10\tnamed\tVBD\t6\trcmod\n",
      "11\tEve\tNNP\t12\tnn\n",
      "12\tKendall\tNNP\t17\tnn\n",
      "13\t(\tNNP\t14\tnn\n",
      "14\tEva\tNNP\t17\tnn\n",
      "15\tMarie\tNNP\t16\tnn\n",
      "16\tSaint\tNNP\t17\tnn\n",
      "17\t)\tNNP\t10\tdobj\n",
      "18\twho\tWP\t19\tnsubj\n",
      "19\tadds\tVBZ\t17\trcmod\n",
      "20\ta\tDT\t21\tdet\n",
      "21\tlot\tNN\t19\tdobj\n",
      "22\tof\tIN\t21\tprep\n",
      "23\tsex\tNN\t24\tnn\n",
      "24\tappeal\tNN\t22\tpobj\n",
      "25\talong\tIN\t19\tprep\n",
      "26\twith\tIN\t25\tpcomp\n",
      "27\tseveral\tJJ\t28\tamod\n",
      "28\tsurprises\tNNS\t26\tpobj\n",
      "29\tto\tTO\t28\tprep\n",
      "30\tthis\tDT\t31\tdet\n",
      "31\twild\tNN\t29\tpobj\n",
      "32\tand\tCC\t31\tcc\n",
      "33\twonderful\tJJ\t34\tamod\n",
      "34\tplot\tNN\t31\tconj\n",
      "35\t.\t.\t6\tpunct\n",
      "-------------------------------------\n",
      "plot is ['wonderful', 'plot'] \n",
      "\n",
      "\n",
      "1\tAt\tIN\t4\tadvmod\n",
      "2\tleast\tJJS\t1\tpobj\n",
      "3\tit\tPRP\t4\tnsubj\n",
      "4\thas\tVBZ\t0\troot\n",
      "5\ta\tDT\t6\tdet\n",
      "6\tplot\tNN\t4\tdobj\n",
      "7\t,\t,\t14\tpunct\n",
      "8\tunlike\tIN\t14\tprep\n",
      "9\ta\tDT\t10\tdet\n",
      "10\tlot\tNN\t8\tpobj\n",
      "11\tof\tIN\t10\tprep\n",
      "12\tthe\tDT\t13\tdet\n",
      "13\tjunk\tNN\t11\tpobj\n",
      "14\tnow\tRB\t4\tdep\n",
      "15\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "plot is [] \n",
      "\n",
      "\n",
      "1\tAbove\tIN\t21\tprep\n",
      "2\taverage\tJJ\t4\tamod\n",
      "3\tmartial\tJJ\t4\tamod\n",
      "4\tarts\tNNS\t1\tpobj\n",
      "5\t,\t,\t1\tpunct\n",
      "6\tbut\tCC\t1\tcc\n",
      "7\tif\tIN\t1\tconj\n",
      "8\tyou\tPRP\t9\tnsubj\n",
      "9\tread\tVBP\t7\tpcomp\n",
      "10\tthe\tDT\t12\tdet\n",
      "11\tplot\tNN\t12\tnn\n",
      "12\tdescription\tNN\t9\tdobj\n",
      "13\ton\tIN\t12\tprep\n",
      "14\tthe\tDT\t16\tdet\n",
      "15\tDVD\tNNP\t16\tnn\n",
      "16\tcase\tNN\t13\tpobj\n",
      "17\tit\tPRP\t20\tnsubj\n",
      "18\t's\tVBZ\t20\tcop\n",
      "19\tobvious\tJJ\t20\tamod\n",
      "20\twhoever\tNN\t1\tdep\n",
      "21\twrote\tVBD\t0\troot\n",
      "22\tthe\tDT\t23\tdet\n",
      "23\tpackaging\tNN\t27\tnsubj\n",
      "24\tdid\tVBD\t27\taux\n",
      "25\tn't\tRB\t27\tneg\n",
      "26\tever\tRB\t27\tadvmod\n",
      "27\twatch\tVB\t21\tccomp\n",
      "28\tthe\tDT\t29\tdet\n",
      "29\tfilm\tNN\t27\tdobj\n",
      "30\t.\t.\t21\tpunct\n",
      "-------------------------------------\n",
      "plot is [] \n",
      "\n",
      "\n",
      "1\tIn\tIN\t10\tprep\n",
      "2\tsummary\tNN\t1\tpobj\n",
      "3\t,\t,\t10\tpunct\n",
      "4\tthe\tDT\t6\tdet\n",
      "5\tmusic\tNN\t6\tnn\n",
      "6\tportion\tNN\t10\tnsubj\n",
      "7\tof\tIN\t6\tprep\n",
      "8\tthis\tDT\t9\tdet\n",
      "9\tDVD\tNNP\t7\tpobj\n",
      "10\trates\tVBZ\t0\troot\n",
      "11\ta\tDT\t14\tdet\n",
      "12\tstrong\tJJ\t14\tamod\n",
      "13\tfive\tCD\t14\tnum\n",
      "14\tstars\tNNS\t10\tdobj\n",
      "15\t,\t,\t10\tpunct\n",
      "16\tbut\tCC\t10\tcc\n",
      "17\tthe\tDT\t19\tdet\n",
      "18\tsilly\tRB\t19\tadvmod\n",
      "19\tplot\tNN\t10\tconj\n",
      "20\tand\tCC\t10\tcc\n",
      "21\tthe\tDT\t22\tdet\n",
      "22\tproblems\tNNS\t26\tnsubj\n",
      "23\twith\tIN\t22\tprep\n",
      "24\tthe\tDT\t25\tdet\n",
      "25\tdisk\tNN\t23\tpobj\n",
      "26\tdiscussed\tVBD\t10\tconj\n",
      "27\tabove\tIN\t26\tprep\n",
      "28\t,\t,\t26\tpunct\n",
      "29\tbring\tVBG\t26\txcomp\n",
      "30\tthis\tDT\t31\tdet\n",
      "31\tDVD\tNNP\t29\tdobj\n",
      "32\tdown\tIN\t29\tadvmod\n",
      "33\tto\tTO\t29\tprep\n",
      "34\ta\tDT\t37\tdet\n",
      "35\tfour\tCD\t37\tnum\n",
      "36\tstar\tNN\t37\tnn\n",
      "37\tproduction\tNN\t33\tpobj\n",
      "-------------------------------------\n",
      "plot is ['silly plot', 'discussed'] \n",
      "\n",
      "\n",
      "1\tUpon\tNNP\t2\tnn\n",
      "2\treflection\tNN\t7\tnsubj\n",
      "3\t,\t,\t7\tpunct\n",
      "4\tthough\tIN\t7\tmark\n",
      "5\t,\t,\t7\tpunct\n",
      "6\tyou\tPRP\t7\tnsubj\n",
      "7\trealize\tVBP\t0\troot\n",
      "8\tthat\tIN\t15\tcomplm\n",
      "9\tsome\tDT\t11\tdet\n",
      "10\tplot\tNN\t11\tnn\n",
      "11\tdetails\tNNS\t15\tnsubj\n",
      "12\treally\tRB\t15\tadvmod\n",
      "13\twere\tVBD\t15\tauxpass\n",
      "14\tn't\tRB\t15\tneg\n",
      "15\tdeveloped\tVBN\t7\tccomp\n",
      "16\tsufficiently\tRB\t15\tadvmod\n",
      "17\t-\t:\t15\tpunct\n",
      "18\tthe\tDT\t21\tdet\n",
      "19\twhole\tJJ\t21\tamod\n",
      "20\tnephew\tNN\t21\tnn\n",
      "21\tthing\tNN\t24\tnsubj\n",
      "22\tbeing\tVBG\t24\tcop\n",
      "23\ta\tDT\t24\tdet\n",
      "24\tcase\tNN\t15\tparataxis\n",
      "25\tin\tIN\t24\tprep\n",
      "26\tpoint\tNN\t25\tpobj\n",
      "27\t.\t.\t7\tpunct\n",
      "-------------------------------------\n",
      "plot is [] \n",
      "\n",
      "\n",
      "1\tWe\tPRP\t2\tnsubj\n",
      "2\twind\tVBP\t0\troot\n",
      "3\tup\tRP\t2\tprt\n",
      "4\tcaring\tVBG\t2\txcomp\n",
      "5\tas\tIN\t4\tprep\n",
      "6\tmuch\tJJ\t5\tpobj\n",
      "7\tabout\tIN\t4\tprep\n",
      "8\tthe\tDT\t9\tdet\n",
      "9\tcharacters\tNNS\t7\tpobj\n",
      "10\tas\tIN\t12\tmark\n",
      "11\twe\tPRP\t12\tnsubj\n",
      "12\tdo\tVBP\t4\tadvcl\n",
      "13\tthe\tDT\t14\tdet\n",
      "14\tplot\tNN\t12\tdobj\n",
      "15\t,\t,\t4\tpunct\n",
      "16\tas\tIN\t18\tmark\n",
      "17\tmuch\tJJ\t18\tadvmod\n",
      "18\timpressed\tVBN\t4\tadvcl\n",
      "19\tby\tIN\t18\tprep\n",
      "20\ttheir\tPRP$\t21\tposs\n",
      "21\tdepth\tNN\t19\tpobj\n",
      "22\tas\tIN\t24\tmark\n",
      "23\twe\tPRP\t24\tnsubj\n",
      "24\tare\tVBP\t18\tadvcl\n",
      "25\tby\tIN\t24\tprep\n",
      "26\tthe\tDT\t27\tdet\n",
      "27\tartwork\tNN\t25\tpobj\n",
      "28\tand\tCC\t27\tcc\n",
      "29\tmusic\tJJ\t27\tconj\n",
      "30\t.\t.\t2\tpunct\n",
      "-------------------------------------\n",
      "plot is [] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'plot'\n",
    "for parsed_sentence in parsed_sentences_plot[:9]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s is %s \\n\\n' % (aspect, opinion) #print 'aspect is opinion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tDT\t3\tdet\n",
      "2\tmain\tJJ\t3\tamod\n",
      "3\tcharacters\tNNS\t16\tnsubj\n",
      "4\tin\tIN\t3\tprep\n",
      "5\tDivided\tNNP\t7\tnn\n",
      "6\tWe\tNNP\t7\tnn\n",
      "7\tFall\tNNP\t4\tpobj\n",
      "8\t,\t,\t7\tpunct\n",
      "9\tJosef\tNNP\t7\tconj\n",
      "10\tand\tCC\t7\tcc\n",
      "11\tMaria\tNNP\t7\tconj\n",
      "12\t,\t,\t3\tpunct\n",
      "13\tare\tVBP\t16\tcop\n",
      "14\ta\tDT\t16\tdet\n",
      "15\tmarried\tJJ\t16\tamod\n",
      "16\tcouple\tNN\t0\troot\n",
      "17\twho\tWP\t18\tnsubj\n",
      "18\tend\tNN\t16\trcmod\n",
      "19\tup\tRP\t18\tprt\n",
      "20\tharboring\tVBG\t18\txcomp\n",
      "21\ta\tDT\t23\tdet\n",
      "22\tJewish\tJJ\t23\tamod\n",
      "23\tacquaintance\tNN\t20\tdobj\n",
      "24\tin\tIN\t23\tprep\n",
      "25\ttheir\tPRP$\t26\tposs\n",
      "26\tcellar\tJJ\t24\tpobj\n",
      "27\tfor\tIN\t20\tprep\n",
      "28\tthree\tCD\t29\tnum\n",
      "29\tyears\tNNS\t27\tpobj\n",
      "30\t.\t.\t16\tpunct\n",
      "-------------------------------------\n",
      "characters are [] \n",
      "\n",
      "\n",
      "1\tWith\tIN\t13\tprep\n",
      "2\ta\tDT\t3\tdet\n",
      "3\tcast\tJJ\t1\tpobj\n",
      "4\tas\tIN\t13\tquantmod\n",
      "5\tlarge\tJJ\t13\tadvcl\n",
      "6\tas\tIN\t5\tprep\n",
      "7\tthis\tDT\t8\tdet\n",
      "8\tone\tCD\t6\tpobj\n",
      "9\t,\t,\t13\tpunct\n",
      "10\tit\tPRP\t13\tnsubj\n",
      "11\t's\tVBZ\t13\tcop\n",
      "12\tonly\tRB\t13\tadvmod\n",
      "13\tnatural\tJJ\t0\troot\n",
      "14\tthat\tIN\t18\tcomplm\n",
      "15\tshort\tJJ\t16\tamod\n",
      "16\tthrift\tNN\t18\tnsubjpass\n",
      "17\tbe\tVB\t18\tauxpass\n",
      "18\tgiven\tVBN\t13\tccomp\n",
      "19\tto\tTO\t18\tprep\n",
      "20\tperipheral\tJJ\t21\tamod\n",
      "21\tcharacters\tNNS\t19\tpobj\n",
      "22\t.\t.\t13\tpunct\n",
      "-------------------------------------\n",
      "characters are ['peripheral'] \n",
      "\n",
      "\n",
      "1\tRocky\tNNP\t7\tnsubj\n",
      "2\t&\tCC\t1\tcc\n",
      "3\tBullwinkle\tNNP\t1\tconj\n",
      "4\tare\tVBP\t7\tcop\n",
      "5\tsuch\tJJ\t7\tamod\n",
      "6\tdear\tJJ\t7\tamod\n",
      "7\tcharacters\tNNS\t0\troot\n",
      "8\t,\t,\t7\tpunct\n",
      "9\tFractured\tNNP\t10\tnn\n",
      "10\tFairytales\tNNP\t7\tappos\n",
      "11\t,\t,\t10\tpunct\n",
      "12\tAesop\tNNP\t10\tconj\n",
      "13\t,\t,\t10\tpunct\n",
      "14\tDudly\tNNP\t15\tnn\n",
      "15\tDoright\tNNP\t10\tconj\n",
      "16\tand\tCC\t10\tcc\n",
      "17\tMr.\tNNP\t18\tnn\n",
      "18\tPeabody\tNNP\t10\tconj\n",
      "19\tadd\tVBD\t7\tpartmod\n",
      "20\tto\tTO\t19\tprep\n",
      "21\tthe\tDT\t23\tdet\n",
      "22\twonderful\tJJ\t23\tamod\n",
      "23\ttime\tNN\t20\tpobj\n",
      "24\tyou\tPRP\t26\tnsubj\n",
      "25\t'll\tMD\t26\taux\n",
      "26\thave\tVB\t23\trcmod\n",
      "27\twatching\tVBG\t7\tpartmod\n",
      "28\tthis\tDT\t30\tdet\n",
      "29\tclassic\tJJ\t30\tamod\n",
      "30\tprogram\tNN\t27\tdobj\n",
      "31\t.\t.\t7\tpunct\n",
      "-------------------------------------\n",
      "characters are ['such', 'dear'] \n",
      "\n",
      "\n",
      "1\tUnconventional\tNNP\t10\tnsubj\n",
      "2\t,\t,\t1\tpunct\n",
      "3\tto\tTO\t4\taux\n",
      "4\tsay\tVB\t1\tinfmod\n",
      "5\tthe\tDT\t6\tdet\n",
      "6\tleast\tJJS\t4\tdobj\n",
      "7\t,\t,\t1\tpunct\n",
      "8\tGame\tNNP\t1\tappos\n",
      "9\t6\tCD\t8\tnum\n",
      "10\thas\tVBZ\t0\troot\n",
      "11\tso\tRB\t12\tadvmod\n",
      "12\tmany\tJJ\t14\tamod\n",
      "13\tinteresting\tJJ\t14\tamod\n",
      "14\tturns\tNNS\t10\tdobj\n",
      "15\tof\tIN\t14\tprep\n",
      "16\tevents\tNNS\t15\tpobj\n",
      "17\t,\t,\t16\tpunct\n",
      "18\tdialog\tNN\t16\tconj\n",
      "19\t,\t,\t10\tpunct\n",
      "20\tand\tCC\t10\tcc\n",
      "21\tcharacters\tNNS\t0\troot\n",
      "22\tin\tIN\t21\tprep\n",
      "23\tit\tPRP\t22\tpobj\n",
      "24\t,\t,\t21\tpunct\n",
      "25\tthat\tIN\t29\tdobj\n",
      "26\tyou\tPRP\t29\tnsubj\n",
      "27\tnever\tRB\t29\tneg\n",
      "28\tget\tVB\t29\tauxpass\n",
      "29\ttired\tVBN\t21\trcmod\n",
      "30\tof\tIN\t29\tprep\n",
      "31\twatching\tNN\t30\tpobj\n",
      "32\tit\tPRP\t31\tdep\n",
      "33\t.\t.\t21\tpunct\n",
      "-------------------------------------\n",
      "characters are [] \n",
      "\n",
      "\n",
      "1\tWhile\tIN\t5\tmark\n",
      "2\tits\tPRP$\t3\tposs\n",
      "3\tcharacters\tNNS\t5\tnsubjpass\n",
      "4\tare\tVBP\t5\tauxpass\n",
      "5\tbased\tVBN\t0\troot\n",
      "6\ton\tIN\t5\tprep\n",
      "7\tthe\tDT\t0\troot\n",
      "8\tthose\tDT\t25\tnsubj\n",
      "9\tdepicted\tVBN\t8\tpartmod\n",
      "10\tin\tIN\t9\tprep\n",
      "11\tWashington\tNNP\t12\tnn\n",
      "12\tIrving\tNNP\t16\tposs\n",
      "13\t's\tPOS\t12\tpossessive\n",
      "14\t``\t``\t16\tpunct\n",
      "15\tThe\tDT\t16\tdet\n",
      "16\tLegend\tNNP\t10\tpobj\n",
      "17\tof\tIN\t16\tprep\n",
      "18\tSleepy\tNNP\t19\tnn\n",
      "19\tHollow\tNNP\t17\tpobj\n",
      "20\t,\t,\t25\tpunct\n",
      "21\t``\t``\t25\tpunct\n",
      "22\tand\tCC\t25\tcc\n",
      "23\tthe\tDT\t24\tdet\n",
      "24\ttale\tNN\t25\tnsubj\n",
      "25\ttakes\tVBZ\t7\trcmod\n",
      "26\tplace\tNN\t25\tdobj\n",
      "27\tin\tIN\t25\tprep\n",
      "28\tSleepy\tNNP\t29\tnn\n",
      "29\tHollow\tNNP\t27\tpobj\n",
      "30\t,\t,\t29\tpunct\n",
      "31\tNew\tNNP\t32\tnn\n",
      "32\tYork\tNNP\t29\tappos\n",
      "33\t,\t,\t29\tpunct\n",
      "34\tthe\tDT\t36\tdet\n",
      "35\tstory\tNN\t36\tnn\n",
      "36\tvaries\tNNS\t29\tappos\n",
      "37\tgreatly\tRB\t29\tadvmod\n",
      "38\tfrom\tIN\t7\tprep\n",
      "39\tthe\tDT\t40\tdet\n",
      "40\tnovel\tNN\t38\tpobj\n",
      "41\t.\t.\t7\terased\n",
      "-------------------------------------\n",
      "characters are [] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tsinging\tNN\t6\tnsubj\n",
      "3\tis\tVBZ\t6\tcop\n",
      "4\tnot\tRB\t6\tneg\n",
      "5\tbad\tJJ\t6\tamod\n",
      "6\t(\tNN\t0\troot\n",
      "7\tthis\tDT\t8\tdet\n",
      "8\tcast\tNN\t9\tnsubj\n",
      "9\thas\tVBZ\t6\trcmod\n",
      "10\tbetter\tJJR\t12\tamod\n",
      "11\ttechnical\tJJ\t12\tamod\n",
      "12\tcontrol\tNN\t9\tdobj\n",
      "13\tof\tIN\t12\tprep\n",
      "14\ttheir\tPRP$\t15\tposs\n",
      "15\tvoices\tNNS\t13\tpobj\n",
      "16\tcompared\tVBN\t12\tprep\n",
      "17\tto\tTO\t16\tpcomp\n",
      "18\tthe\tDT\t19\tdet\n",
      "19\tcast\tNN\t17\tpobj\n",
      "20\tin\tIN\t19\tprep\n",
      "21\tLes\tNNP\t23\tnn\n",
      "22\tMiz\tNNP\t23\tnn\n",
      "23\t)\tNNP\t20\tpobj\n",
      "24\tbut\tCC\t19\tcc\n",
      "25\tthe\tDT\t26\tdet\n",
      "26\tchoreography\tNN\t19\tconj\n",
      "27\tis\tVBZ\t28\tcop\n",
      "28\ttrite\tJJ\t6\tamod\n",
      "29\tand\tCC\t28\tcc\n",
      "30\tunimaginative\tJJ\t28\tconj\n",
      "31\t,\t,\t6\tpunct\n",
      "32\tand\tCC\t6\tcc\n",
      "33\tthe\tDT\t36\tdet\n",
      "34\tacting\tJJ\t36\tamod\n",
      "35\tso\tRB\t36\tadvmod\n",
      "36\tjuvenille\tJJ\t6\tconj\n",
      "37\tthat\tIN\t40\tcomplm\n",
      "38\tyou\tPRP\t40\tnsubj\n",
      "39\t'd\tMD\t40\taux\n",
      "40\tthink\tVB\t36\tccomp\n",
      "41\tthe\tDT\t42\tdet\n",
      "42\tcharacters\tNNS\t44\tnsubjpass\n",
      "43\tare\tVBP\t44\tauxpass\n",
      "44\tretarded\tVBN\t40\tccomp\n",
      "45\t.\t.\t6\tpunct\n",
      "-------------------------------------\n",
      "characters are [] \n",
      "\n",
      "\n",
      "1\tI\tPRP\t3\tnsubjpass\n",
      "2\twas\tVBD\t3\tauxpass\n",
      "3\tsuprised\tVBN\t0\troot\n",
      "4\tto\tTO\t5\taux\n",
      "5\tfind\tVB\t3\txcomp\n",
      "6\tthat\tIN\t15\tcomplm\n",
      "7\tfor\tIN\t15\tprep\n",
      "8\tbeing\tVBG\t9\tcop\n",
      "9\tcartoons\tNNS\t7\tpcomp\n",
      "10\t,\t,\t15\tpunct\n",
      "11\tthese\tDT\t14\tdet\n",
      "12\thot\tJJ\t14\tamod\n",
      "13\twheel\tNN\t14\tnn\n",
      "14\tmovies\tNNS\t15\tnsubj\n",
      "15\thave\tVBP\t5\tccomp\n",
      "16\tcharacters\tNNS\t15\tdobj\n",
      "17\twith\tIN\t16\tprep\n",
      "18\tdepth\tNN\t17\tpobj\n",
      "19\tand\tCC\t18\tcc\n",
      "20\tstories\tNNS\t18\tconj\n",
      "21\twith\tIN\t20\tprep\n",
      "22\tan\tDT\t24\tdet\n",
      "23\tactual\tJJ\t24\tamod\n",
      "24\tplot\tNN\t21\tpobj\n",
      "25\t.\t.\t3\tpunct\n",
      "-------------------------------------\n",
      "characters are [] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tmovie\tNN\t3\tnsubj\n",
      "3\tstarts\tVBZ\t0\troot\n",
      "4\toff\tRP\t3\tprt\n",
      "5\twith\tIN\t3\tprep\n",
      "6\tan\tDT\t8\tdet\n",
      "7\tinteresting\tJJ\t8\tamod\n",
      "8\tnaration\tNN\t5\tpobj\n",
      "9\t,\t,\t3\tpunct\n",
      "10\tbut\tCC\t3\tcc\n",
      "11\t2/3\tCD\t12\tnum\n",
      "12\tcharacters\tNNS\t14\tnsubjpass\n",
      "13\tare\tVBP\t14\tauxpass\n",
      "14\tkilled\tVBN\t3\tconj\n",
      "15\tafter\tIN\t14\tprep\n",
      "16\tabout\tIN\t17\tquantmod\n",
      "17\t30\tCD\t18\tnum\n",
      "18\tseconds\tNNS\t15\tpobj\n",
      "19\tand\tCC\t3\tcc\n",
      "20\twe\tPRP\t22\tnsubj\n",
      "21\tnever\tRB\t22\tneg\n",
      "22\tget\tVB\t3\tconj\n",
      "23\tto\tTO\t24\taux\n",
      "24\tknow\tVB\t22\txcomp\n",
      "25\tthem\tPRP\t24\tdobj\n",
      "26\tor\tCC\t34\tcc\n",
      "27\tcare\tVBP\t34\tparataxis\n",
      "28\tabout\tIN\t27\tprep\n",
      "29\tthem\tPRP\t28\tpobj\n",
      "30\t,\t,\t27\tpunct\n",
      "31\twhich\tWDT\t34\tnsubj\n",
      "32\tis\tVBZ\t34\tcop\n",
      "33\tn't\tRB\t34\tneg\n",
      "34\tgood\tJJ\t22\tadvcl\n",
      "35\t.\t.\t3\tpunct\n",
      "-------------------------------------\n",
      "characters are [] \n",
      "\n",
      "\n",
      "1\t2nd\tCD\t4\tnsubj\n",
      "2\t,\t,\t4\tpunct\n",
      "3\tit\tPRP\t4\tnsubj\n",
      "4\toffers\tVBZ\t0\troot\n",
      "5\tsome\tDT\t7\tdet\n",
      "6\tnew\tJJ\t7\tamod\n",
      "7\ttwist\tNN\t4\tdobj\n",
      "8\tin\tIN\t7\tprep\n",
      "9\tthat\tDT\t15\tdobj\n",
      "10\tthe\tDT\t12\tdet\n",
      "11\tother\tJJ\t12\tamod\n",
      "12\tcharacters\tNNS\t15\tnsubj\n",
      "13\tdo\tVBP\t15\taux\n",
      "14\tn't\tRB\t15\tneg\n",
      "15\tknow\tVB\t8\tpcomp\n",
      "16\tat\tIN\t15\tprep\n",
      "17\tfirst\tJJ\t16\tpobj\n",
      "18\twho\tWP\t20\tdobj\n",
      "19\tJason\tNNP\t20\tnsubj\n",
      "20\tis\tVBZ\t17\trcmod\n",
      "21\t,\t,\t17\tpunct\n",
      "22\tand\tCC\t17\tcc\n",
      "23\tin\tIN\t17\tconj\n",
      "24\tone\tCD\t23\tpobj\n",
      "25\twell\tRB\t26\tadvmod\n",
      "26\tdrawn\tVBN\t24\tpartmod\n",
      "27\tscene\tNN\t31\tnn\n",
      "28\teven\tRB\t31\tadvmod\n",
      "29\twe\tPRP\t31\tnsubj\n",
      "30\tthe\tDT\t31\tdet\n",
      "31\taudience\tNN\t34\tnsubj\n",
      "32\tdo\tVBP\t34\taux\n",
      "33\tn't\tRB\t34\tneg\n",
      "34\tknow\tVB\t26\tccomp\n",
      "35\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "characters are ['other'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'characters'\n",
    "\n",
    "for parsed_sentence in parsed_sentences_characters[:9]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s are %s \\n\\n' % (aspect, opinion) #print 'aspect are opinion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tDT\t2\tdet\n",
      "2\tplot\tNN\t4\tnsubj\n",
      "3\tis\tVBZ\t4\taux\n",
      "4\tuninvolving\tVBG\t0\troot\n",
      "5\tand\tCC\t4\tcc\n",
      "6\tincoherent\tNN\t11\tnsubjpass\n",
      "7\t,\t,\t11\tpunct\n",
      "8\tthe\tDT\t9\tdet\n",
      "9\tcinematography\tNN\t11\tnsubjpass\n",
      "10\tis\tVBZ\t11\tauxpass\n",
      "11\tcramped\tVBN\t4\tconj\n",
      "12\tand\tCC\t11\tcc\n",
      "13\tcompletely\tRB\t14\tadvmod\n",
      "14\tlacking\tVBG\t11\tconj\n",
      "15\tin\tIN\t14\tprep\n",
      "16\tstyle\tNN\t15\tpobj\n",
      "17\t,\t,\t21\tpunct\n",
      "18\tthe\tDT\t19\tdet\n",
      "19\tmusic\tNN\t21\tnsubj\n",
      "20\tis\tVBZ\t21\tcop\n",
      "21\tweak\tJJ\t4\tdep\n",
      "22\tduring\tIN\t4\tprep\n",
      "23\ta\tDT\t24\tdet\n",
      "24\ttime\tNN\t22\tpobj\n",
      "25\tthat\tWDT\t26\tnsubj\n",
      "26\tspawned\tVBD\t24\trcmod\n",
      "27\tso\tRB\t28\tadvmod\n",
      "28\tmany\tJJ\t30\tamod\n",
      "29\tmemorable\tJJ\t30\tamod\n",
      "30\tsoundtracks\tNNS\t26\tdobj\n",
      "31\tand\tCC\t30\tcc\n",
      "32\tthe\tDT\t33\tdet\n",
      "33\tdirection\tNN\t30\tconj\n",
      "34\tlacks\tVBZ\t4\taux\n",
      "35\tany\tDT\t37\tdet\n",
      "36\tfocus\tNN\t37\tnn\n",
      "37\twhatsoever\tNN\t4\tdobj\n",
      "38\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "cinematography is ['completely lacking'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tsets\tNNS\t5\tnsubj\n",
      "3\tare\tVBP\t5\taux\n",
      "4\tquite\tRB\t5\tadvmod\n",
      "5\tamazing\tVBG\t0\troot\n",
      "6\t,\t,\t5\tpunct\n",
      "7\tand\tCC\t5\tcc\n",
      "8\tthe\tDT\t9\tdet\n",
      "9\tcinematography\tNN\t12\tnsubj\n",
      "10\tis\tVBZ\t12\tcop\n",
      "11\tabsolutely\tRB\t12\tadvmod\n",
      "12\tbeautiful\tJJ\t5\tconj\n",
      "13\t.\t.\t5\tpunct\n",
      "-------------------------------------\n",
      "cinematography is ['absolutely-beautiful'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t5\tdet\n",
      "2\tother\tJJ\t5\tamod\n",
      "3\tmore\tJJR\t4\tdep\n",
      "4\ttragic\tJJ\t5\tamod\n",
      "5\tflaw\tNN\t18\tnsubj\n",
      "6\t,\t,\t18\tpunct\n",
      "7\tsince\tIN\t11\tmark\n",
      "8\tit\tPRP\t11\tnsubj\n",
      "9\tis\tVBZ\t11\tcop\n",
      "10\ta\tDT\t11\tdet\n",
      "11\tmovie\tNN\t18\tadvcl\n",
      "12\tabout\tIN\t11\tprep\n",
      "13\tdance\tNN\t12\tpobj\n",
      "14\t,\t,\t15\tpunct\n",
      "15\tis\tVBZ\t18\tparataxis\n",
      "16\tthe\tDT\t17\tdet\n",
      "17\tcinematography\tNN\t15\tnsubj\n",
      "18\t.\t.\t0\troot\n",
      "-------------------------------------\n",
      "cinematography is [] \n",
      "\n",
      "\n",
      "1\tWhile\tIN\t8\tmark\n",
      "2\tit\tPRP\t8\tnsubj\n",
      "3\t's\tVBZ\t8\tcop\n",
      "4\tcertainly\tRB\t8\tadvmod\n",
      "5\tnot\tRB\t8\tneg\n",
      "6\tthe\tDT\t8\tdet\n",
      "7\tworst\tJJS\t8\tamod\n",
      "8\tcinematography\tNN\t12\tadvcl\n",
      "9\tever\tRB\t8\tadvmod\n",
      "10\t,\t,\t12\tpunct\n",
      "11\t(\t:\t12\tpunct\n",
      "12\tcheck\tNN\t0\troot\n",
      "13\tout\tIN\t19\tmark\n",
      "14\t``\t``\t19\tpunct\n",
      "15\tthe\tDT\t16\tdet\n",
      "16\tLimey\tNNP\t19\tnsubj\n",
      "17\t``\t``\t19\tpunct\n",
      "18\tnext\tJJ\t19\tamod\n",
      "19\ttime\tNN\t12\tdep\n",
      "20\tyou\tPRP\t21\tnsubj\n",
      "21\twant\tVBP\t19\trcmod\n",
      "22\ta\tDT\t23\tdet\n",
      "23\theadache\tNN\t21\tdobj\n",
      "24\t.\t.\t12\tpunct\n",
      "-------------------------------------\n",
      "cinematography is ['not worst'] \n",
      "\n",
      "\n",
      "1\tSo\tIN\t13\tdep\n",
      "2\t,\t,\t13\tpunct\n",
      "3\tdespite\tIN\t13\tprep\n",
      "4\tthe\tDT\t6\tdet\n",
      "5\tbeautiful\tJJ\t6\tamod\n",
      "6\tcinematography\tNN\t3\tpobj\n",
      "7\t,\t,\t13\tpunct\n",
      "8\tthis\tDT\t9\tdet\n",
      "9\tmovie\tNN\t13\tnsubj\n",
      "10\tis\tVBZ\t13\tcop\n",
      "11\ta\tDT\t13\tdet\n",
      "12\thopeless\tNN\t13\tnn\n",
      "13\tmuddle\tNN\t0\troot\n",
      "14\t.\t.\t13\tpunct\n",
      "-------------------------------------\n",
      "cinematography is ['beautiful'] \n",
      "\n",
      "\n",
      "1\tHaving\tVBG\t2\taux\n",
      "2\tsaid\tVBD\t0\troot\n",
      "3\tthat\tIN\t8\tcomplm\n",
      "4\t,\t,\t8\tpunct\n",
      "5\tthe\tDT\t6\tdet\n",
      "6\tcinematography\tNN\t8\tnsubj\n",
      "7\twas\tVBD\t8\taux\n",
      "8\tstunning\tVBG\t2\tccomp\n",
      "9\tand\tCC\t8\tcc\n",
      "10\tthe\tDT\t11\tdet\n",
      "11\tmusic\tNN\t13\tnsubj\n",
      "12\talmost\tRB\t13\tadvmod\n",
      "13\tmade\tVBD\t8\tconj\n",
      "14\tthe\tDT\t16\tdet\n",
      "15\tfilm\tNN\t16\tnn\n",
      "16\tworth\tNN\t13\tdobj\n",
      "17\twatching\tVBG\t13\txcomp\n",
      "18\t.\t.\t2\tpunct\n",
      "-------------------------------------\n",
      "cinematography is ['almost made'] \n",
      "\n",
      "\n",
      "1\tAnd\tCC\t4\tcc\n",
      "2\tthat\tDT\t4\tnsubj\n",
      "3\t's\tVBZ\t4\tcop\n",
      "4\tsurprising\tJJ\t0\troot\n",
      "5\t,\t,\t4\tpunct\n",
      "6\tgiven\tVBN\t4\tprep\n",
      "7\tthat\tIN\t11\tcomplm\n",
      "8\tthe\tDT\t9\tdet\n",
      "9\tmovie\tNN\t11\tnsubjpass\n",
      "10\tis\tVBZ\t11\tauxpass\n",
      "11\thelmed\tVBN\t6\tpcomp\n",
      "12\tby\tIN\t11\tprep\n",
      "13\t``\t``\t12\tpunct\n",
      "14\tRomeo\tNNP\t16\tnn\n",
      "15\tMust\tNNP\t16\tnn\n",
      "16\tDie\tNNP\t18\tdep\n",
      "17\t``\t``\t18\tpunct\n",
      "18\tdirector\tNN\t20\tnn\n",
      "19\tAndrzej\tNNP\t20\tnn\n",
      "20\tBartkowiak\tNNP\t12\tpobj\n",
      "21\t,\t,\t20\tpunct\n",
      "22\twho\tWP\t24\tnsubj\n",
      "23\talso\tRB\t24\tadvmod\n",
      "24\tdid\tVBD\t20\trcmod\n",
      "25\tthe\tDT\t26\tdet\n",
      "26\tcinematography\tNN\t24\tdobj\n",
      "27\tfor\tIN\t26\tprep\n",
      "28\tSpeed\tNNP\t27\tpobj\n",
      "29\t,\t,\t28\tpunct\n",
      "30\tLethal\tNNP\t31\tnn\n",
      "31\tWeapon\tNNP\t28\tappos\n",
      "32\t4\tCD\t31\tnum\n",
      "33\t,\t,\t28\tpunct\n",
      "34\tand\tCC\t28\tcc\n",
      "35\tDevil\tNNP\t37\tposs\n",
      "36\t's\tPOS\t35\tpossessive\n",
      "37\tAdvocate\tNNP\t28\tconj\n",
      "38\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "cinematography is [] \n",
      "\n",
      "\n",
      "1\t``\t``\t0\troot\n",
      "2\tThe\tDT\t4\tdet\n",
      "3\tMummy\tNNP\t4\tnn\n",
      "4\tReturns\tNNP\t6\tnsubj\n",
      "5\t``\t``\t6\tpunct\n",
      "6\thas\tVBZ\t9\taux\n",
      "7\ta\tDT\t8\tdet\n",
      "8\tlot\tNN\t9\tnsubj\n",
      "9\tgoing\tVBG\t0\troot\n",
      "10\tfor\tIN\t9\tprep\n",
      "11\tit\tPRP\t10\tpobj\n",
      "12\t:\t:\t15\tpunct\n",
      "13\tA\tDT\t15\tdet\n",
      "14\tgreat\tJJ\t15\tamod\n",
      "15\tcast\tNN\t9\tdep\n",
      "16\t,\t,\t15\tpunct\n",
      "17\tlocations\tNNS\t15\tappos\n",
      "18\t,\t,\t15\tpunct\n",
      "19\tcostumes\tVBZ\t15\tdep\n",
      "20\t,\t,\t19\tpunct\n",
      "21\tcinematography\tNN\t15\tdep\n",
      "22\t,\t,\t15\tpunct\n",
      "23\tF/X\tJJ\t15\tamod\n",
      "24\t,\t,\t15\tpunct\n",
      "25\tetc\tNN\t15\tdep\n",
      "26\t.\t.\t9\terased\n",
      "-------------------------------------\n",
      "cinematography is [] \n",
      "\n",
      "\n",
      "1\tHowever\tRB\t17\tadvmod\n",
      "2\t,\t,\t17\tpunct\n",
      "3\tthe\tDT\t5\tdet\n",
      "4\tawful\tJJ\t5\tamod\n",
      "5\tcinematography\tNN\t17\tnsubj\n",
      "6\t,\t,\t5\tpunct\n",
      "7\thorrific\tJJ\t8\tamod\n",
      "8\tediting\tNN\t5\tconj\n",
      "9\t,\t,\t5\tpunct\n",
      "10\tand\tCC\t5\tcc\n",
      "11\tterrible\tJJ\t13\tamod\n",
      "12\tproduction\tNN\t13\tnn\n",
      "13\tvalue\tNN\t5\tconj\n",
      "14\tof\tIN\t13\tprep\n",
      "15\tthis\tDT\t16\tdet\n",
      "16\tfilm\tNN\t14\tpobj\n",
      "17\tmake\tVBP\t0\troot\n",
      "18\timpossible\tJJ\t17\tacomp\n",
      "19\tfor\tIN\t22\tmark\n",
      "20\tme\tPRP\t22\tnsubj\n",
      "21\tto\tTO\t22\taux\n",
      "22\tfathom\tVB\t17\tadvcl\n",
      "23\twhy\tWRB\t27\tadvmod\n",
      "24\tpeople\tNNS\t27\tnsubj\n",
      "25\tdid\tVBD\t27\taux\n",
      "26\tn't\tRB\t27\tneg\n",
      "27\tburn\tVB\t22\tccomp\n",
      "28\tdown\tIN\t27\tadvmod\n",
      "29\tthe\tDT\t30\tdet\n",
      "30\ttheaters\tNNS\t27\tdobj\n",
      "31\twhen\tWRB\t33\tadvmod\n",
      "32\tthey\tPRP\t33\tnsubj\n",
      "33\trealized\tVBD\t27\tadvcl\n",
      "34\tthis\tDT\t35\tdet\n",
      "35\tfilm\tNN\t38\tnsubj\n",
      "36\twas\tVBD\t38\taux\n",
      "37\tn't\tRB\t38\tneg\n",
      "38\tgetting\tVBG\t33\tccomp\n",
      "39\tany\tDT\t40\tdet\n",
      "40\tbetter\tJJR\t38\tdobj\n",
      "41\t.\t.\t17\tpunct\n",
      "-------------------------------------\n",
      "cinematography is ['awful'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'cinematography'\n",
    "\n",
    "for parsed_sentence in parsed_sentences_cinematography[:9]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s is %s \\n\\n' % (aspect, opinion) #print 'aspect is opinion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tDT\t2\tdet\n",
      "2\tperformances\tNNS\t4\tnsubj\n",
      "3\tare\tVBP\t4\tcop\n",
      "4\tthin\tJJ\t0\troot\n",
      "5\tand\tCC\t4\tcc\n",
      "6\tuneven\tJJ\t4\tconj\n",
      "7\t,\t,\t4\tpunct\n",
      "8\tthe\tDT\t9\tdet\n",
      "9\tplot\tNN\t11\tnsubj\n",
      "10\tis\tVBZ\t11\tcop\n",
      "11\tinconsistent\tNN\t4\tconj\n",
      "12\t,\t,\t11\tpunct\n",
      "13\tand\tCC\t11\tcc\n",
      "14\tthe\tDT\t15\tdet\n",
      "15\tdialogue\tNN\t16\tnsubj\n",
      "16\tsounds\tVBZ\t11\tconj\n",
      "17\tlike\tIN\t16\tprep\n",
      "18\tit\tPRP\t17\tpobj\n",
      "19\twas\tVBD\t20\tauxpass\n",
      "20\tpulled\tVBN\t4\tdep\n",
      "21\tfrom\tIN\t20\tprep\n",
      "22\tfortune\tNN\t23\tnn\n",
      "23\tcookies\tNNS\t21\tpobj\n",
      "24\tmade\tVBN\t23\tpartmod\n",
      "25\tby\tIN\t24\tprep\n",
      "26\tthe\tDT\t27\tdet\n",
      "27\tcast\tJJ\t25\tpobj\n",
      "28\tof\tIN\t27\tprep\n",
      "29\tFriends\tNNPS\t28\tpobj\n",
      "30\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "dialogue is [] \n",
      "\n",
      "\n",
      "1\tHer\tPRP$\t2\tposs\n",
      "2\tdialogue\tNN\t4\tnsubjpass\n",
      "3\tis\tVBZ\t4\tauxpass\n",
      "4\tkept\tVBN\t0\troot\n",
      "5\tshort\tJJ\t4\tacomp\n",
      "6\tand\tCC\t5\tcc\n",
      "7\tsimple\tJJ\t5\tconj\n",
      "8\t,\t,\t4\tpunct\n",
      "9\tand\tCC\t4\tcc\n",
      "10\twhen\tWRB\t13\tadvmod\n",
      "11\tshe\tPRP\t13\tnsubj\n",
      "12\tdoes\tVBZ\t13\taux\n",
      "13\thave\tVB\t33\tadvcl\n",
      "14\tmore\tJJR\t15\tmwe\n",
      "15\tthan\tIN\t16\tquantmod\n",
      "16\t3\tCD\t17\tnum\n",
      "17\tlines\tNNS\t13\tdobj\n",
      "18\tat\tIN\t13\tadvmod\n",
      "19\tonce\tRB\t18\tadvmod\n",
      "20\t,\t,\t33\tpunct\n",
      "21\tthe\tDT\t23\tdet\n",
      "22\tcomplete\tJJ\t23\tamod\n",
      "23\tlack\tNN\t33\tnsubj\n",
      "24\tof\tIN\t23\tprep\n",
      "25\tfeeling\tVBG\t24\tpcomp\n",
      "26\t,\t,\t25\tpunct\n",
      "27\texpression\tNN\t25\tdobj\n",
      "28\t&\tCC\t27\tcc\n",
      "29\temotion\tNN\t27\tconj\n",
      "30\tin\tIN\t25\tprep\n",
      "31\ther\tPRP$\t32\tposs\n",
      "32\tvoice\tNN\t30\tpobj\n",
      "33\tmakes\tVBZ\t4\tconj\n",
      "34\tyou\tPRP\t36\tnsubj\n",
      "35\tconsider\tJJR\t36\taux\n",
      "36\tturning\tVBG\t33\tccomp\n",
      "37\toff\tRP\t36\tprt\n",
      "38\tthe\tDT\t39\tdet\n",
      "39\tmovie\tNN\t36\tdobj\n",
      "40\tand\tCC\t36\tcc\n",
      "41\tdoing\tVBG\t36\tconj\n",
      "42\tsomething\tNN\t43\tnn\n",
      "43\tworthwhile\tNN\t41\tdobj\n",
      "44\t,\t,\t36\tpunct\n",
      "45\tlike\tIN\t36\tprep\n",
      "46\tcleaning\tNN\t45\tpobj\n",
      "47\tout\tIN\t36\tprep\n",
      "48\tthe\tDT\t49\tdet\n",
      "49\tfridge\tNN\t47\tpobj\n",
      "50\tor\tCC\t36\tcc\n",
      "51\tpicking\tVBG\t36\tconj\n",
      "52\tat\tIN\t51\tprep\n",
      "53\tyour\tPRP$\t54\tposs\n",
      "54\thang-nails\tNNS\t52\tpobj\n",
      "55\t.\t.\t4\tpunct\n",
      "-------------------------------------\n",
      "dialogue is ['makes'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tdialogue\tNN\t3\tnsubj\n",
      "3\tis\tVBZ\t0\troot\n",
      "4\tat\tIN\t3\tprep\n",
      "5\ta\tDT\t8\tdet\n",
      "6\tmuch\tJJ\t8\tamod\n",
      "7\tlower\tNN\t8\tnn\n",
      "8\tvolume\tNN\t4\tpobj\n",
      "9\tthan\tIN\t8\tprep\n",
      "10\tthe\tDT\t11\tdet\n",
      "11\tmusic\tNN\t9\tpobj\n",
      "12\tand\tCC\t3\tcc\n",
      "13\tsound\tVB\t14\tdep\n",
      "14\teffects\tNNS\t3\tconj\n",
      "15\t,\t,\t3\tpunct\n",
      "16\tmaking\tVBG\t3\txcomp\n",
      "17\tit\tPRP\t20\tnsubj\n",
      "18\timpossible\tJJ\t20\tdep\n",
      "19\tto\tTO\t20\taux\n",
      "20\tview\tVB\t16\txcomp\n",
      "21\twithout\tIN\t20\tprep\n",
      "22\tconstantly\tRB\t23\tadvmod\n",
      "23\ttinkering\tVBG\t21\tpcomp\n",
      "24\t.\t.\t3\tpunct\n",
      "-------------------------------------\n",
      "dialogue is ['effects'] \n",
      "\n",
      "\n",
      "1\tHaving\tVBG\t2\taux\n",
      "2\tmentioned\tVBN\t0\troot\n",
      "3\tit\tPRP\t6\tnsubj\n",
      "4\t's\tVBZ\t6\tcop\n",
      "5\tgood\tJJ\t6\tamod\n",
      "6\tpoints\tNNS\t2\tccomp\n",
      "7\tI\tPRP\t8\tnsubj\n",
      "8\thave\tVBP\t6\tccomp\n",
      "9\tto\tTO\t10\taux\n",
      "10\tsay\tVB\t8\txcomp\n",
      "11\tI\tPRP\t14\tnsubjpass\n",
      "12\twas\tVBD\t14\tauxpass\n",
      "13\tenormously\tRB\t14\tadvmod\n",
      "14\tdisappointed\tVBN\t10\tccomp\n",
      "15\twith\tIN\t14\tprep\n",
      "16\tthe\tDT\t17\tdet\n",
      "17\tdialogue\tNN\t15\tpobj\n",
      "18\tand\tCC\t17\tcc\n",
      "19\tsome\tDT\t17\tconj\n",
      "20\tof\tIN\t19\tprep\n",
      "21\tthe\tDT\t23\tdet\n",
      "22\tstory\tNN\t23\tnn\n",
      "23\tadaptation\tNN\t20\tpobj\n",
      "24\tto\tTO\t25\taux\n",
      "25\tscreen\tVB\t23\tinfmod\n",
      "26\t.\t.\t2\terased\n",
      "-------------------------------------\n",
      "dialogue is [] \n",
      "\n",
      "\n",
      "1\tFor\tIN\t16\tprep\n",
      "2\tthe\tDT\t4\tdet\n",
      "3\tmost\tJJS\t4\tamod\n",
      "4\tpart\tNN\t1\tpobj\n",
      "5\tthe\tDT\t7\tdet\n",
      "6\tbad\tJJ\t7\tamod\n",
      "7\tdialogue\tNN\t4\tdep\n",
      "8\tand\tCC\t7\tcc\n",
      "9\tsome\tDT\t7\tconj\n",
      "10\tof\tIN\t9\tprep\n",
      "11\tthe\tDT\t13\tdet\n",
      "12\tterrible\tJJ\t13\tamod\n",
      "13\tacting\tNN\t10\tpobj\n",
      "14\tin\tIN\t4\tprep\n",
      "15\tparts\tNNS\t14\tpobj\n",
      "16\twere\tVBD\t0\troot\n",
      "17\tjust\tRB\t19\tadvmod\n",
      "18\ttoo\tRB\t19\tadvmod\n",
      "19\tpainful\tJJ\t16\tnsubj\n",
      "20\tto\tTO\t21\taux\n",
      "21\tsit\tVB\t19\tinfmod\n",
      "22\tthrough\tIN\t21\tprt\n",
      "23\t.\t.\t16\tpunct\n",
      "-------------------------------------\n",
      "dialogue is ['bad'] \n",
      "\n",
      "\n",
      "1\tAll\tDT\t3\tpredet\n",
      "2\tthe\tDT\t3\tdet\n",
      "3\tcharacters\tNNS\t5\tnsubj\n",
      "4\twere\tVBD\t5\tcop\n",
      "5\tseverely\tRB\t0\troot\n",
      "6\tand\tCC\t5\tcc\n",
      "7\ttragically\tRB\t8\tadvmod\n",
      "8\t2-dimensional\tJJ\t5\tconj\n",
      "9\twith\tIN\t8\tprep\n",
      "10\tnone\tNN\t9\tpobj\n",
      "11\tof\tIN\t10\tprep\n",
      "12\tthe\tDT\t13\tdet\n",
      "13\tdepth\tNN\t11\tpobj\n",
      "14\tyou\tPRP\t15\tnsubj\n",
      "15\tget\tVBP\t10\trcmod\n",
      "16\tfrom\tIN\t15\tprep\n",
      "17\treading\tNN\t16\tpobj\n",
      "18\tthe\tDT\t20\tdet\n",
      "19\tgraphic\tJJ\t20\tamod\n",
      "20\tnovel\tNN\t16\ttmod\n",
      "21\tand\tCC\t15\tcc\n",
      "22\tvery\tRB\t23\tadvmod\n",
      "23\tlittle\tRB\t24\tamod\n",
      "24\tattention\tNN\t15\tconj\n",
      "25\twas\tVBD\t26\tauxpass\n",
      "26\tpaid\tVBN\t5\tdep\n",
      "27\tto\tTO\t26\tprep\n",
      "28\tdeveloping\tVBG\t27\tpcomp\n",
      "29\tbetter\tJJR\t30\tamod\n",
      "30\tdialogue\tNN\t28\tdobj\n",
      "31\tsuitable\tJJ\t30\tamod\n",
      "32\tfor\tIN\t30\tprep\n",
      "33\ta\tDT\t36\tdet\n",
      "34\tbig\tJJ\t36\tamod\n",
      "35\tscreen\tNN\t36\tnn\n",
      "36\texperience\tNN\t32\tpobj\n",
      "37\t.\t.\t5\tpunct\n",
      "-------------------------------------\n",
      "dialogue is ['better', 'suitable'] \n",
      "\n",
      "\n",
      "1\tPurists\tNNS\t11\tnsubj\n",
      "2\tand\tCC\t1\tcc\n",
      "3\trabid\tVBD\t1\tconj\n",
      "4\tfans\tNNS\t3\tdobj\n",
      "5\tof\tIN\t4\tprep\n",
      "6\tthe\tDT\t8\tdet\n",
      "7\tgraphic\tJJ\t8\tamod\n",
      "8\tnovel\tNN\t5\tpobj\n",
      "9\tmay\tMD\t11\taux\n",
      "10\tbe\tVB\t11\tcop\n",
      "11\tcapable\tJJ\t0\troot\n",
      "12\tof\tIN\t11\tprep\n",
      "13\tsomehow\tNN\t14\tnn\n",
      "14\tfinding\tNN\t12\tpobj\n",
      "15\ta\tDT\t16\tdet\n",
      "16\tway\tNN\t14\tdep\n",
      "17\tto\tTO\t18\taux\n",
      "18\tlook\tVB\t16\tinfmod\n",
      "19\tpast\tJJ\t18\tacomp\n",
      "20\tall\tDT\t26\tdet\n",
      "21\tthe\tDT\t23\tdet\n",
      "22\tterrible\tJJ\t23\tamod\n",
      "23\tacting\tNN\t26\tnn\n",
      "24\tand\tCC\t23\tcc\n",
      "25\tawful\tJJ\t23\tconj\n",
      "26\tdialogue\tNN\t18\tdobj\n",
      "27\tthat\tWDT\t29\tnsubj\n",
      "28\tis\tVBZ\t29\tcop\n",
      "29\trife\tVBP\t26\trcmod\n",
      "30\tthroughout\tIN\t29\tprep\n",
      "31\tthe\tDT\t32\tdet\n",
      "32\tfilm\tNN\t30\tpobj\n",
      "33\t.\t.\t11\tpunct\n",
      "-------------------------------------\n",
      "dialogue is [] \n",
      "\n",
      "\n",
      "1\tSome\tDT\t7\tnsubjpass\n",
      "2\tof\tIN\t1\tprep\n",
      "3\tmy\tPRP$\t5\tposs\n",
      "4\tfavorite\tJJ\t5\tamod\n",
      "5\tactors\tNNS\t2\tpobj\n",
      "6\tare\tVBP\t7\tauxpass\n",
      "7\twasted\tVBN\t21\tccomp\n",
      "8\t,\t,\t21\tpunct\n",
      "9\teither\tDT\t21\tnsubj\n",
      "10\tby\tIN\t9\tprep\n",
      "11\tbeing\tVBG\t12\tauxpass\n",
      "12\tgiven\tVBN\t10\tpcomp\n",
      "13\tcliched\tJJ\t14\tamod\n",
      "14\tdialogue\tNN\t12\tdobj\n",
      "15\t,\t,\t21\tpunct\n",
      "16\tover\tIN\t21\tprep\n",
      "17\tacting\tJJ\t16\tpobj\n",
      "18\t,\t,\t21\tpunct\n",
      "19\tor\tCC\t21\tcc\n",
      "20\tbeing\tVBG\t21\tauxpass\n",
      "21\tput\tVBN\t0\troot\n",
      "22\tthere\tEX\t21\tdobj\n",
      "23\tmerely\tRB\t21\tadvmod\n",
      "24\tfor\tIN\t21\tprep\n",
      "25\ttheir\tPRP$\t27\tposs\n",
      "26\tbrand\tNN\t27\tnn\n",
      "27\tname\tNN\t24\tpobj\n",
      "28\twith\tIN\t21\tprep\n",
      "29\tno\tDT\t30\tdet\n",
      "30\tscript\tNN\t28\tpobj\n",
      "31\tto\tTO\t32\taux\n",
      "32\tsupport\tVB\t21\txcomp\n",
      "33\ttheir\tPRP$\t34\tposs\n",
      "34\tpresence\tNN\t32\tdobj\n",
      "35\t.\t.\t21\tpunct\n",
      "-------------------------------------\n",
      "dialogue is ['cliched'] \n",
      "\n",
      "\n",
      "1\tThe\tDT\t2\tdet\n",
      "2\tresolution\tNN\t5\tnsubj\n",
      "3\tis\tVBZ\t5\tcop\n",
      "4\tan\tDT\t5\tdet\n",
      "5\tafterthought\tNN\t0\troot\n",
      "6\t(\t:\t5\tpunct\n",
      "7\ta\tDT\t9\tdet\n",
      "8\tquick\tNN\t9\tnn\n",
      "9\tfix\tNN\t5\tdep\n",
      "10\tin\tIN\t9\tprep\n",
      "11\tthe\tDT\t12\tdet\n",
      "12\textreme\tNN\t10\tpobj\n",
      "13\t)\t:\t5\tpunct\n",
      "14\t,\t,\t5\tpunct\n",
      "15\tand\tCC\t5\tcc\n",
      "16\tbarely\tRB\t18\tadvmod\n",
      "17\teven\tRB\t18\tadvmod\n",
      "18\tmentioned\tVBN\t5\tconj\n",
      "19\tin\tIN\t18\tprep\n",
      "20\tabout\tIN\t22\tamod\n",
      "21\ttwo\tCD\t22\tnum\n",
      "22\tlines\tNNS\t19\tpobj\n",
      "23\tof\tIN\t22\tprep\n",
      "24\tdialogue\tNN\t23\tpobj\n",
      "25\tafter\tIN\t18\tprep\n",
      "26\tthe\tDT\t28\tdet\n",
      "27\tartificial\tJJ\t28\tamod\n",
      "28\tdanger\tNN\t25\tpobj\n",
      "29\tis\tVBZ\t33\taux\n",
      "30\t(\t:\t33\tpunct\n",
      "31\tnarrowly\tRB\t33\tadvmod\n",
      "32\t,\t,\t33\tpunct\n",
      "33\twhew\tVB\t5\tdep\n",
      "34\t!\t.\t5\tpunct\n",
      "-------------------------------------\n",
      "dialogue is [] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect = 'dialogue'\n",
    "for parsed_sentence in parsed_sentences_dialogue[:9]:\n",
    "    # for each occurence of the aspect token in the sentence:\n",
    "    for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "        \n",
    "        opinion = opinion_extractor(aspect_token, parsed_sentence) # call opinion_extractor and store result in 'opinion'.\n",
    "        print parsed_sentence # print the entire parsed sentence.\n",
    "        print '-------------------------------------' # separator\n",
    "        print '%s is %s \\n\\n' % (aspect, opinion) #print 'aspect is opinion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output of the code above it appears that my opinion extractor performs extremely poorly when applied to sentences in the Amazon DVD corpus. The opinion extractor performed as follows for the respective aspects:\n",
    "\n",
    "                                    Plot 2/10 opinions extracted correctly\n",
    "                                    \n",
    "                                    Characters 2/9 opinions extracted correctly\n",
    "                                    \n",
    "                                    cinematography 4/9 opinion extracted correctly\n",
    "                                    \n",
    "                                    dialogue 2/9 opinions extracted correctly\n",
    "\n",
    "I belive that the poor performance of my opinion extractor when using samples from the amazon DVD corpus compared to the relative success when using the example test set, is down to a few reasons. Mainly I think that this is a direct result of tailoring my extensions to work well with the example testing set. However in the wider corpus there is a much greater variation on how sentences can be structured while still conveying the same sentiment and opinions. For example the sentences 'The plot made watching this movie a complete waste of time.' and 'Watching this movie was a complete waste of time because of the plot' both mean the same thing but are structured in different ways. The other reason that I think may be responsible for my opinion extractor failing is closely linked with the last reason in that the testing set was provided with the intent of being used for the extensions in labs 8 and 9. I feel that had I implemented the extra extensions, my opinion extractor would have performed better as these address more complex gramatical structure of sentences. For example while my extractor function has been show to correctly extract the opinion 'not dull' from 'the plot wasn't dull' it would fall down for 'I think the plot wasn't dull'. But with added functionality in the form of further extensions, this could be achieved. While it is possible that subset of the errors are down to POS tagger errors or parser errors, I feel that the moajority of the blame lies with the opinion extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b> Website Proposal</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this section I will put forward a proposal as to how I would go about creating a website that automatically produces summaries of what people think about various aspects of a DVD.</p><br>\n",
    "<p>Firstly from the retailer's point of view, implementing NLP techniques such as those I have discussed in this report would allow them to profile their users based on what aspects of a DVD they view as important. This could be done by enforcing a policy that in order to leave a review on a product, the reviewer must be a registered member of the site. This would allow all reviews to be assiciated with one particular person. The reviews could then be run through NLE software which extracts information about which aspects the reviewer talks about the most in their reviews. All DVDs could then have their reviews also run through similar NLP software that extracts the total number of positive reviews for each aspect. The user could then be matched up with DVDs which have a high number of positive reviews mentioning the aspects that they consider most valuable based on their previous reviews that they have themselves written. These DVDs could be presented to the reviewer on page shown after their initial log in. This would be a mutually beneficial process to implement as the reviewer is presented with DVDs which have good reviews on aspects that they deem important and as a result of this the retailer would see an increase in revenue, supposing that good software was implemented and the reviewer has faith in the system providing accurate suggestions.</p><br>\n",
    "<p>Another way that NLP software could be used in an online DVD retail business involves the retailer and the studio that makes the movies. Again the motivation behind this would be a monetary one. The retailer could implement an opinion extractor tailored specifically to DVDs, and in a simalar fashion to the previous idea take a count of which aspect is spoken about the most but the key difference is to also take negative reviews into account as well as positive reviews. This information would be extremely valuable to the film studios who may or may not consider making sequels or other movies that fall into the same genre to gauge what the target audience deems important fot that type of movie. This would allow them to make improvements to areas that are found to be lacking by reviewers and also to see what they are doing right that they may want to keep doing in the future. The retailer could charge the studios a fee for processing and providing this information, therefore again increasing revenue.</p><br>\n",
    "<p>As with any form of commercial software implementation problems may arise. Accuracy stands out as the biggest hurdle in this situation, as POS tagging and Parsing are not trivial tasks and errors do occur. If a sentence has been incorrectly POS tagged (for example a word with high ambiguity is tagged as one pos class when in the context concerned it should be tagged as another) then it will be incorrectly parsed and then as an effect of that will not have the correct opinion extracted from it. The challenge is to ensure that tagging and parsing errors are minimised so that the opinion extractor has the best chance of being accurate. Also the opinion extractor itself needs to be robust enough to cope with the variety of gramatical structure present in the language it is being applied to.</p>\n",
    "<p>This leads on to my final point that is people writing online reviews for products may not have the best literacy skills leading to human errors in the way of grammar, spelling and punctuation, leading to the errors mentioned above. One way around this would be to implement some sort of algortithm that holds common typing, spelling, grammar and punctiation errors and checks each word when it is being tokenised. This however would undoubtably involve an high computational cost unless some sort of dynamic approach similar to a sequence alignment algorithm.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> References </b>\n",
    "\n",
    "Jurafsky, Dan, and James H Martin. Speech And Language Processing. Upper Saddle River, N.J.: Pearson Prentice Hall, 2009. Print.\n",
    "\n",
    "Kübler, Sandra, Ryan McDonald, and Joakim Nivre. Dependency Parsing. [San Rafael, Calif.]: Morgan & Claypool Publishers, 2009. Print."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
